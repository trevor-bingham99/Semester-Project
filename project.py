import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import seaborn as sns
import json
import numpy as np
import time
from itertools import groupby
import plotly.express as px

#This function is to help scroll through the data table with Selenium, as only a few rows load at a time, and the others are hidden until you scroll.
def scroll(driver, timeout):
    # get scroll height
    last_height = driver.execute_script("return document.body.scrollHeight")

    while True:
        for i in range(timeout):
            # scroll down
            driver.find_element(By.XPATH,"/html/body/div[7]/div[2]/div/div[1]/div[3]/div/div/div[2]/div/table").send_keys(Keys.END)

            # wait for page to load
            time.sleep(1)

        # get new scroll height and compare to last height
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            # If heights are the same it will exit the function
            break
        last_height = new_height

#This URL is one generated by the sites Query service, and returns the data in a json script I will parse
url = 'https://services1.arcgis.com/99lidPhWCzftIe9K/arcgis/rest/services/HousingUnitInventory/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=false&outSR=4326&f=json'

#Here I have downloaded the CSV they have of the data, but I will also be scraping the data as well, to show others ways to get it. 
df = pd.read_csv('./Utah_Housing_Unit_Inventory.csv')

driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

driver.get(url)
pre = driver.find_element(By.TAG_NAME,"pre").text
data = json.loads(pre)
#This takes the relevant part from the json file to load into a DataFrame.
df = pd.json_normalize(data['features'])
#Cleaning some column names
new_column_names = {col: col.replace('attributes.', '') for col in df.columns}
df.rename(columns=new_column_names, inplace=True)
df.set_index('OBJECTID', inplace=True)
df.reset_index().drop(columns='OBJECTID')

url1= 'https://opendata.gis.utah.gov/datasets/utah::utah-housing-unit-inventory/about'

driver.get(url1)

# This function goes through the table, scrolls down until there are no more rows loaded. 
# As there are many rows, it takes a very long time to get them all, so I also added a limit of 10000
# rows as a stopping point, and even that can take about an hour, so you can shrink it as you want.
def get_all_rows(driver, table_locator):
    # Scroll to the bottom of the page to ensure all rows are loaded
    last_row_count = 0

    while True:
        # Record the current number of rows
        current_row_count = len(driver.find_elements(By.XPATH, table_locator + "//tr"))

        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # Wait for a short time to let new rows load
        scroll(driver,10)

        # Get the new number of rows
        new_row_count = len(driver.find_elements(By.XPATH, table_locator + "//tr"))

        # If the number of rows hasn't increased or it hits your chosen limit, break the loop
        if new_row_count == current_row_count or new_row_count==10000:
            break

        # Update the last row count
        last_row_count = new_row_count

    html = driver.page_source

    # Parse the HTML using BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the table in the parsed HTML
    table = soup.find('table')

    # Check if the table was found
    if table:
        # Extract all rows from the table
        all_rows = table.find_all('tr')
        return all_rows
    else:
        print("Table not found.")
        return []    

#This is navigating to the correct page with the data table, and then sorts the data by year, descending to try and mix up the order of my sample a bit.
driver.find_element(By.ID,"ember99").click()

driver.find_element(By.ID,"ember75-title").click()
driver.find_element(By.ID,"ember75-title").click()
# XPath of the table
table_locator = "/html/body/div[7]/div[2]/div/div[1]/div[3]/div/div/div[2]/div/table"

all_rows = get_all_rows(driver, table_locator)

driver.close()

header_columns = [span.get_text(strip=True) for span in all_rows[0].find_all('span', {'role': 'button'})]

# Initialize an empty list to store row data
all_rows_data = []

# Iterate through each row
for row_html in all_rows:
    # Extract data from each cell in the row
    data = [cell.get_text(strip=True) for cell in row_html.find_all('td', {'role': 'gridcell'})]
    
    # Create a dictionary for the row
    row_dict = dict(zip(header_columns, data))
    
    # Append the row data to the list
    all_rows_data.append(row_dict)

# Create a DataFrame from the list of row data
df1 = pd.DataFrame(all_rows_data)
df1.dropna(inplace=True)


#Here I am combiing the few thousand rows I scraped with selenium, and the few thousand rows the query json loaded, and checking for duplicates
combined_df = pd.concat([df, df1], ignore_index=True)
combined_df.drop_duplicates(subset='UNIT_ID', keep='first',inplace=True)

columns_to_convert_to_float = ['UNIT_ID', 'UNIT_COUNT', 'DUA','ACRES','TOT_BD_FT2','TOT_VALUE','APX_BLT_YR','BLT_DECADE','Shape__Area','Shape__Length','IS_OUG']

# Convert specified columns to float
combined_df[columns_to_convert_to_float] = combined_df[columns_to_convert_to_float].replace(",","",regex=True).replace('', 0).astype(float)

#EDA Visualizations
scat1 = px.scatter(combined_df, x='TOT_VALUE', y='TOT_BD_FT2', color='SUBTYPE', title='Value vs Square Footage',opacity=.5)
scat1.show()

scat2 = px.scatter(combined_df[(combined_df['APX_BLT_YR']!=0) & (combined_df['TOT_BD_FT2']<9000000)], x='APX_BLT_YR', y='TOT_BD_FT2', color='SUBTYPE', title='Year Built vs Square Footage',opacity=.5)
scat2.show()

scat3 = px.scatter(combined_df[(combined_df['APX_BLT_YR']!=0) & (combined_df['TOT_VALUE']<300000000) & (combined_df['ACRES']<500)], x='ACRES', y='TOT_VALUE', color='SUBTYPE', title='Acres of Land vs Total Value',opacity=.5)
scat3.show()

scat4 = px.scatter(combined_df[(combined_df['APX_BLT_YR']!=0) & (combined_df['TOT_BD_FT2']<9000000)], x='TOT_VALUE', y='TOT_BD_FT2', title='Price vs Square Footage',opacity=.5)
scat4.show()

category_counts = combined_df['SUBTYPE'].value_counts()

# Exclude categories with a count of 0
filtered_counts = category_counts[category_counts != 0]

# Create the bar chart with the filtered counts
bar = px.bar(x=filtered_counts.index, y=filtered_counts.values, text=filtered_counts.values,color=filtered_counts.index,title='Counts of Each Housing Subtype')
bar.show()

hist = px.histogram(
    combined_df[combined_df['TOT_VALUE']<9000000],
    x='TOT_VALUE',
    title='Distribution of Total Values',
    nbins=50,  
    opacity=0.7
)
hist.show()

#Code to write the data to csv, but I have commented it out so it doesn't run each time. 
#combined_df.to_csv('./HousingData.csv')